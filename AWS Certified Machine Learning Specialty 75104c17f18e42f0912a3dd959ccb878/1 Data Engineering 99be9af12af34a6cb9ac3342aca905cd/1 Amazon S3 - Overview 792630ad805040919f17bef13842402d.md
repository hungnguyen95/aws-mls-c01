# 1. Amazon S3 - Overview

- Amazon S3 allows people to store objects (files) in “buckets” (directories)
- Buckets must have a globally unique name
- Objects (files) have a Key. The key is the FULL path:
    - <my_bucket>/my_file.txt
    - <my_bucket>/my_folder1/another_folder/my_file.txt
- This will be interesting when we look at partitioning
- Max object size is 5TB
- Object Tags (key / value pair – up to 10) – useful for security /lifecycle

---

# AWS S3 for Machine Learning

- Backbone for many AWS ML services (example: SageMaker)
- Create a “Data Lake"
    - Infinite size, no provisioning
    - 99.999999999% durability
    - Decoupling of storage (S3) to compute (EC2, Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, and AWS Glue)
- Centralized Architecture (all files in one place)
- Object storage => supports any file format (**Data Lake**)
- Common formats for ML: CSV, JSON, Parquet, ORC, Avro, Protobuf

---

# AWS S3 Data Partitioning

- Pattern for speeding up range queries (ex: AWS Athena)
- By Date: `s3://bucket/my-dataset/year/month/day/hour/data_00.csv`
- By Product: `s3://bucket/my-data-set/product-id/data_32.csv`
- You can define whatever partitioning strategy you like!
- Data partitioning will be handled by some tools we use (e.g. AWS Glue)