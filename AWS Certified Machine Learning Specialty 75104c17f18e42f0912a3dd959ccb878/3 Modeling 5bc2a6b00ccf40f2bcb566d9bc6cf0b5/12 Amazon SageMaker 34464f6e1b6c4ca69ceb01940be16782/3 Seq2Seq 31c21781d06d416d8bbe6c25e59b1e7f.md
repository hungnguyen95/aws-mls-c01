# 3. Seq2Seq

## Seq2Seq: What’s it for?

- Input is a sequence of tokens, output is a sequence of tokens
- Machine Translation
- Text summarization
- Speech to text
- Implemented with RNN’s and CNN’s with attention

---

## Seq2Seq: What training input does it expect?

- RecordIO-Protobuf
    - Tokens must be integers (this is unusual, since most algorithms want floating point data.)
- Start with tokenized text files
- Convert to protobuf using sample code
    - Packs into integer tensors with vocabulary files
    - A lot like the TF/IDF lab we did earlier.
- Must provide training data, validation data, and vocabulary files.

---

## Seq2Seq: How is it used?

- Training for machine translation can take days, even on SageMaker
- Pre-trained models are available
- See the example notebook
- Public training datasets are available for specific translation tasks

---

## Seq2Seq: Important Hyperparameters

- Batch_size
- Optimizer_type (adam, sgd, rmsprop)
- Learning_rate
- Num_layers_encoder
- Num_layers_decoder
- Can optimize on:
    - Accuracy
        - Vs. provided validation dataset
    - BLEU score
        - Compares against multiple reference translations
    - Perplexity
        - Cross-entropy

---

## Seq2Seq: Instance Types

- Can only use GPU instance types (P3 for example)
- Can only use a single machine for training
    - But can use multi-GPU’s on one machine