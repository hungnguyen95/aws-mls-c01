# 14. K-Means

## K-Means: What’s it for?

- Unsupervised clustering
- Divide data into K groups, where members of a group are as similar as possible to each other
    - You define what “similar” means
    - Measured by Euclidean distance
- Web-scale K-Means clustering

![14%20K-Means%20a51a9145a0304c3d93bcab5b25813a40/Untitled.png](14%20K-Means%20a51a9145a0304c3d93bcab5b25813a40/Untitled.png)

---

## K-Means: What training input does it expect?

- Train channel, optional test
    - Train ShardedByS3Key, test FullyReplicated
- recordIO-protobuf or CSV
- File or Pipe on either

---

## K-Means: How is it used?

- Every observation mapped to n-dimensional space (n = number of features)
- Works to optimize the center of K clusters
    - “extra cluster centers” may be specified to improve accuracy (which end up getting reduced to k)
    - K = k*x
- Algorithm:
    - Determine initial cluster centers
        - Random or k-means++ approach
        - K-means++ tries to make initial clusters far apart
    - Iterate over training data and calculate cluster centers
    - Reduce clusters from K to k
        - Using Lloyd’s method with kmeans++

---

## K-Means: Important Hyperparameters

- K!
    - Choosing K is tricky
    - Plot within-cluster sum of squares as function of K
    - Use “elbow method”Basically optimize for tightness of clusters
- Mini_batch_size
- Extra_center_factor
- Init_method

---

## K-Means: Instance Types

- CPU or GPU, but CPU recommended
    - Only one GPU per instance usedon GPU
    - So use p*.xlarge if you’re going to use GPU